[
    {
        "title": "EduMUSE",
        "startDate": "May 2025",
        "endDate": "Present",
        "link": "https://github.com/iamsorenl/EduMUSE",
        "muiIcon": "Abc",
        "description": "- Built a sophisticated multi-agent educational AI system that orchestrates specialized CrewAI agents through a modern React interface, transforming how students interact with academic content\n- Developed a modular flow architecture enabling systematic comparison of AI methodologies—web search, LLM-based synthesis, and hybrid retrieval—accessible through an intuitive PDF viewer with intelligent text selection\n- Implemented a specialized agent ecosystem including ConceptExtractor for academic analysis, SummaryWriter for adaptive content creation, and AssessmentDesigner for pedagogically sound quiz generation\n- Integrated Flask-based PDF upload API with CORS support and react-pdf-highlighter, allowing users to upload academic documents and interact with AI agents for real-time content generation\n- Successfully integrated podcast generation feature that transforms academic content into engaging audio discussions between AI hosts, expanding the system's multimodal capabilities",
        "tools": [
            "Python",
            "CrewAI",
            "React",
            "Flask",
            "CORS",
            "react-pdf-highlighter",
            "AI Agents",
            "Educational Technology",
            "Multimodal AI"
        ]
    },
    {
        "title": "Multi-Agent AI Orchestration Platform with Web Integration Demo",
        "startDate": "May 2025",
        "endDate": "June 2025",
        "link": "https://github.com/iamsorenl/Autogen_Chat_Demo",
        "muiIcon": "Abc",
        "description": "- Built a sophisticated multi-agent conversational AI system that orchestrates specialized AI agents through a modern web interface, combining AutoGen's MagenticOneGroupChat with real-time frontend integration\n- Developed custom WebSocket-based communication layer to seamlessly route user interactions between React frontend and AutoGen backend, eliminating terminal dependencies for true web-native AI conversations\n- Implemented specialized agent ecosystem including ScientistAgent for analytical responses, ArtistAgent for creative perspectives, and MediaHandlerAgent for multimedia content processing\n- Solved complex input routing challenges by creating a custom UserProxyAgent integration that channels human-in-the-loop interactions through the web interface\n- Integrated Flask-based media upload API with CORS support and achieved clean, real-time multi-perspective AI responses with intelligent message filtering and robust error handling",
        "tools": [
            "Python",
            "AutoGen",
            "React",
            "Material-UI",
            "WebSocket",
            "Flask",
            "OpenAI GPT-4",
            "CORS",
            "AsyncIO",
            "Multi-Agent Systems"
        ]
    },
    {
        "title": "UCSC RAG Chatbot — A Retrieval-Augmented Assistant for Student Support",
        "startDate": "January 2025",
        "endDate": "March 2025",
        "link": "https://github.com/shannonrumsey/UCSC_RAG",
        "muiIcon": "Abc",
        "description": "- Built a custom chatbot to answer questions about the UCSC NLP Master's Program and related university services using Retrieval-Augmented Generation (RAG) with LangChain, ChromaDB, and LLaMA 3.2\n- Integrated Gemini 2.0 Flash for post-response evaluation to ensure factual accuracy and reduce hallucinations by checking relevance, coherence, and bias\n- Developed a Swift-based iOS app with Firebase for real-time access, chat history, and user authentication\n- Achieved 97% human-evaluated accuracy on a custom 150-question dataset and 71% strict accuracy under Gemini evaluation\n- Tested across three LLaMA model sizes (1B, 3B, 8B) and retrieved answers from 77 scraped UCSC-affiliated web pages",
        "tools": [
            "Python",
            "LangChain",
            "ChromaDB",
            "Gemini API",
            "Swift",
            "Firebase",
            "LLaMA 3.2",
            "Sentence-Transformers"
        ]
    },
    {
        "title": "Robustness and Domain Adaptation in Transformer-Based Question Answering",
        "startDate": "February 2025",
        "endDate": "February 2025",
        "link": "https://github.com/iamsorenl/Transformer_QA",
        "muiIcon": "Abc",
        "description": "- Analyzed RoBERTa's robustness across in-domain, adversarial, and out-of-domain QA tasks using SQuAD 2.0 and the Covid-QA biomedical dataset\n- Applied Low-Rank Adaptation (LoRA) to fine-tune RoBERTa efficiently, reducing trainable parameters while improving accuracy on specialized biomedical text\n- Introduced sliding window inference and CLS-token masking for long and unanswerable contexts, enhancing answer alignment and minimizing false positives\n- Achieved +6.41 EM / +14.84 F1 on Covid-QA dev and +5.33 EM / +10.54 F1 on test, demonstrating strong domain transfer with adapter-based tuning\n- Identified failure modes in QA under ambiguity and contradiction, informing best practices for robust transformer deployment",
        "tools": [
            "Python",
            "PyTorch",
            "Hugging Face Transformers",
            "Adapter-Transformers",
            "SQuAD 2.0",
            "Covid-QA",
            "LoRA"
        ]
    },
    {
        "title": "Sentiment Analysis Minibatching",
        "startDate": "January 2025",
        "endDate": "February 2025",
        "link": "https://github.com/iamsorenl/Sentiment-Analysis-Minibatching",
        "muiIcon": "Abc",
        "description": "- Developed sentiment classifiers using Logistic Regression and LSTM models on the IMDB movie review dataset, exploring the impact of batch size and learning rate on model performance\n- Implemented dynamic mini-batch training with PyTorch and spaCy, handling over 50,000 variable-length reviews with custom tokenization and data loading\n- Benchmarked configurations across batch sizes and learning rates, achieving up to 90.4% validation accuracy with LR and 90.1% with LSTM\n- Highlighted efficiency–performance trade-offs, showing that intermediate batch sizes (e.g., 16) yielded optimal convergence and generalization\n- Demonstrated that smaller batch sizes increased training time but preserved beneficial gradient noise for model robustness",
        "tools": [
            "Python",
            "PyTorch",
            "spaCy",
            "scikit-learn",
            "matplotlib",
            "IMDB Dataset"
        ]
    },
    {
        "title": "Modernized ELIZA Rogerian Therapist Chatbot",
        "startDate": "January 2025",
        "endDate": "January 2025",
        "link": "https://github.com/iamsorenl/BetterELIZAbot",
        "muiIcon": "Abc",
        "description": "- Enhanced the classic ELIZA chatbot with NLP techniques to better handle humor, misspellings, contractions, and flirtation while preserving its therapeutic tone\n- Implemented a modular spell-checker using TextBlob and SpaCy to correct typos without altering named entities or disrupting context\n- Designed custom rules for laughter and flirtatious input, prompting deeper dialogue or redirection to maintain professional tone\n- Developed contraction-aware lemmatization to improve pattern matching while respecting ELIZA's rule-based structure\n- Increased chatbot realism and user engagement by addressing ambiguous, emotional, or non-serious inputs through modular NLP enhancements",
        "tools": [
            "Python",
            "TextBlob",
            "SpaCy",
            "Regex",
            "Rule-Based NLP"
        ]
    },
    {
        "title": "Neural Machine Translation: Subword vs. Word-Level Tokenization",
        "startDate": "January 2025",
        "endDate": "January 2025",
        "link": "https://github.com/iamsorenl/Neural-Machine-Translation",
        "muiIcon": "Abc",
        "description": "- Built French-to-English NMT models using Transformer and CNN architectures on the IWSLT 2013 dataset within the Fairseq framework\n- Compared Byte Pair Encoding (BPE) and Moses whole-word tokenization to evaluate effects on training speed, vocabulary size, and translation quality\n- Tuned hyperparameters (dropout, model depth) to optimize BLEU scores and reduce overfitting\n- Conducted both quantitative (SacreBLEU) and qualitative analysis to assess output fluency and semantic accuracy\n- Found that whole-word tokenization achieved a top BLEU score of 28.98, while BPE improved OOV handling and convergence speed",
        "tools": [
            "Python",
            "Fairseq",
            "SacreBLEU",
            "Moses Tokenizer",
            "BPE",
            "Bash"
        ]
    },
    {
        "title": "POS Tagging with Hidden Markov Models and Viterbi Algorithm",
        "startDate": "November 2024",
        "endDate": "December 2024",
        "link": "https://github.com/iamsorenl/HMM-POS-Tagger",
        "muiIcon": "Abc",
        "description": "- Developed a POS tagging system using a Hidden Markov Model (HMM) and implemented the Viterbi algorithm for sequence decoding.\n- Designed and implemented log-space computations for numerical stability, addressing challenges like data sparsity and unseen words using fallback probabilities.\n- Leveraged dynamic programming for efficient computation and implemented structured backpointer mechanisms for accurate sequence reconstruction.\n- Evaluated the system using token-level accuracy and F1-scores to ensure reliability and effectiveness.",
        "tools": [
            "Python",
            "Log-Space Programming",
            "Dynamic Programming",
            "HMM",
            "Viterbi Algorithm"
        ]
    },
    {
        "title": "Airline Sentiment Preprocessing and Modeling",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/Airline-Sentiment-Preprocessing-and-Modeling",
        "muiIcon": "Abc",
        "description": "- Developed a pipeline for sentiment analysis on the Twitter US Airline Sentiment dataset, focusing on data preprocessing, custom tokenization, and machine learning classification.\n- Applied advanced text cleaning techniques, including regex-based tokenization, lemmatization, and emoji normalization.\n- Trained a support vector machine (SVM) with TF-IDF features, achieving 80.89% test accuracy through preprocessing optimizations and ablation studies.\n- Delivered actionable insights into sentiment trends across airlines and user behavior using visualizations.",
        "tools": [
            "Python",
            "Scikit-learn",
            "Pandas",
            "Regex",
            "Matplotlib"
        ]
    },
    {
        "title": "Building and Evaluating N-Gram Language Models",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/ngram_project",
        "muiIcon": "Abc",
        "description": "- Built N-Gram models (Unigram, Bigram, Trigram) and Interpolated N-Gram models to predict word sequences and measure perplexity on a subset of the One Billion Word Language Modeling Benchmark dataset.\n- Implemented linear interpolation smoothing and tunable OOV handling to address unseen data and improve model generalization.\n- Optimized interpolation weights (λ1, λ2, λ3) to enhance contextual predictions and reduce perplexity.\n- Analyzed model performance across datasets and demonstrated the effectiveness of interpolation for handling sparse data.",
        "tools": [
            "Python",
            "Numpy",
            "Custom N-Gram Modeling",
            "Perplexity Analysis"
        ]
    },
    {
        "title": "Feature Engineering for Multiclass Classification",
        "startDate": "October 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/feature_engineering_multiclass_classification_SVM_LR_DT",
        "muiIcon": "Abc",
        "description": "- Developed a text classification pipeline for an e-commerce dataset, leveraging feature engineering techniques and multi-class classification models such as SVM, Logistic Regression, and Decision Tree.\n- Implemented GloVe embeddings, Sublinear TF-IDF, and Bag-of-Words features to optimize model performance.\n- Conducted hyperparameter tuning and evaluated performance using metrics like macro-average F1-scores, confusion matrices, and ROC curves.\n- Delivered models with high accuracy and efficient feature representations for text data.",
        "tools": [
            "Python",
            "Scikit-learn",
            "Pandas",
            "GloVe Embeddings",
            "TF-IDF"
        ]
    },
    {
        "title": "Slot Tagging of Natural Language Utterances",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/Slot-Tagging-of-Natural-Language-Utterances",
        "muiIcon": "Abc",
        "description": "- Designed and implemented a BiLSTM with attention mechanism for slot tagging tasks, achieving 75.37% accuracy and F1 scores of 0.950 (sklearn) and 0.839 (seqeval).\n- Integrated pre-trained GloVe embeddings and created custom embedding matrices for semantic token representations.\n- Tuned hyperparameters, including dropout rates and learning rates, to improve model performance and generalization.\n- Evaluated token- and sequence-level metrics to capture complex token dependencies.",
        "tools": [
            "Python",
            "PyTorch",
            "GloVe",
            "Seqeval",
            "Scikit-learn"
        ]
    },
    {
        "title": "Transformer Language Model on Penn Treebank Dataset",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/Language-Modeling-on-Penn-Treebank",
        "muiIcon": "Abc",
        "description": "- Developed a Transformer Encoder model for autoregressive language modeling on the Penn Treebank dataset, reducing test perplexity from 83.35 to 39.11.\n- Optimized model architecture with sinusoidal positional encoding and multi-head attention, improving generalization on small datasets.\n- Conducted extensive hyperparameter tuning to balance model complexity and training efficiency.\n- Evaluated predictive accuracy across training, validation, and test sets, validating the model's scalability for constrained datasets.",
        "tools": [
            "Python",
            "PyTorch",
            "Numpy",
            "Pandas",
            "Scikit-learn"
        ]
    },
    {
        "title": "Feature Engineering and Sentiment Analysis",
        "startDate": "October 2024",
        "endDate": "October 2024",
        "link": "https://github.com/iamsorenl/Feature-Engineering-and-Sentiment-Analysis",
        "muiIcon": "Abc",
        "description": "- Built a text classification pipeline for Amazon and IMDb datasets, exploring feature engineering techniques and comparing custom and scikit-learn models.\n- Implemented custom Naive Bayes, SVM, Decision Tree, and Logistic Regression classifiers with Laplacian smoothing and N-gram features.\n- Achieved 89% accuracy with SVM + TF-IDF on Amazon reviews and 87% validation accuracy with Logistic Regression + Bigrams on IMDb reviews.\n- Leveraged advanced metrics like binary and macro F1-scores to evaluate model performance.",
        "tools": [
            "Python",
            "Scikit-learn",
            "Numpy",
            "Matplotlib",
            "Bag-of-Words",
            "TF-IDF"
        ]
    },
    {
        "title": "Relation Extraction from Natural Language using PyTorch",
        "startDate": "October 2024",
        "endDate": "October 2024",
        "link": "https://github.com/iamsorenl/Relation-Extraction-from-Natural-Language-using-PyTorch",
        "muiIcon": "Abc",
        "description": "- Developed a Multi-Layer Perceptron (MLP) model to extract core relations from natural language utterances using the Freebase schema.\n- Utilized static embeddings from spaCy and trained the model with K-fold cross-validation for robust performance across folds.\n- Applied MultiLabelSoftMarginLoss to handle multi-label predictions effectively, achieving an average F1-score of ~0.91 and accuracy of ~0.86.\n- Optimized the architecture with batch normalization and dropout layers, demonstrating the model's utility in knowledge graph enrichment and conversational AI.",
        "tools": [
            "Python",
            "PyTorch",
            "SpaCy",
            "MultiLabelSoftMarginLoss",
            "K-fold Cross-Validation"
        ]
    }
]
